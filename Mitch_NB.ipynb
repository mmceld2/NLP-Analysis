{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src import *\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer, RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression , SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split , cross_validate , GridSearchCV, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline , Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, recall_score, f1_score\n",
    "from nltk.corpus import stopwords as sw\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.tokenize import  word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "pd.set_option ('display.max_colwidth', None)\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_scores(results):\n",
    "    '''Return Train/Test Mean Score from a Cross Val'''\n",
    "    return print(f'''Mean Train Score: {results['train_score'].mean()}\n",
    "Mean Test Score: {results['test_score'].mean()}''')\n",
    "\n",
    "def metrics(y_true,y_preds):\n",
    "    '''Gives recall,precision,accuracy,F1 score and confusion matrix'''\n",
    "    return print(f'''Recall Score: {recall_score(y_true,y_preds,average='micro')}\n",
    "Precision Score: {precision_score(y_true,y_preds,average='micro')}\n",
    "Accuracy Score:{accuracy_score(y_true,y_preds)}\n",
    "F1 Score: {f1_score(y_true,y_preds,average='micro')}\n",
    "Confusion Matrix: \n",
    "{confusion_matrix(y_true,y_preds)}''')\n",
    "\n",
    "def gs_mean_scores(results):\n",
    "    '''Return Train and Test Scores from a grid search'''\n",
    "    return print(f'''Mean Train Score: {results.cv_results_['mean_train_score'].mean()}'\n",
    "Mean Test Score: {results.cv_results_['mean_test_score'].mean()}''')\n",
    "\n",
    "def company_search(test):\n",
    "    if test == 'iphone':\n",
    "        return 'apple'\n",
    "    elif test == 'apple':\n",
    "        return 'apple'\n",
    "    elif test == 'ipad':\n",
    "        return 'apple'\n",
    "    elif test == 'ipad or iphone app':\n",
    "        return 'apple'\n",
    "    elif test == 'itunes':\n",
    "        return 'apple'\n",
    "    elif test == 'other apple product or service':\n",
    "        return 'apple'\n",
    "    elif test == 'ios':\n",
    "        return 'apple'\n",
    "    elif test == 'ipadquot':\n",
    "        return 'apple'\n",
    "    elif test == 'applequot':\n",
    "        return 'apple'\n",
    "    elif test == 'iphonequot':\n",
    "        return 'apple'\n",
    "    elif test == 'quotiphone':\n",
    "        return 'apple'\n",
    "    elif test == 'iosquot':\n",
    "        return 'apple'\n",
    "    elif test =='quotipad':\n",
    "        return 'apple'\n",
    "    elif test == 'quotapple':\n",
    "        return 'apple'\n",
    "    elif test =='google':\n",
    "        return 'google'\n",
    "    elif test == 'android':\n",
    "        return 'google'\n",
    "    elif test == 'other google product or service':\n",
    "        return 'google'\n",
    "    elif test == 'android app':\n",
    "        return 'google'\n",
    "    elif test == 'quotgoogle':\n",
    "        return 'google'\n",
    "    elif test == 'googlequot':\n",
    "        return 'google'\n",
    "    elif test == 'androidquot':\n",
    "        return 'google'\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                               1\n",
       "emotion_in_tweet_is_directed_at                       5802\n",
       "is_there_an_emotion_directed_at_a_brand_or_product       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/judge-1377884607_tweet_product_company.csv',encoding='latin1')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5802 missing values telling what the tweet is directed at. A way to combat this would be to look at the tweet and look for keywords such as iphone or ipad and assign who it is directed to that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     . @wesley83 i have a 3g iphone . after 3 hrs tweeting at #rise_austin , it was dead ! i need to upgrade . plugin stations at #sxsw .\n",
       "1          @jessedee know about @fludapp ? awesome ipad / iphone app that you'll likely appreciate for its design . also , they're giving free ts at #sxsw\n",
       "2                                                                        @swonderlin can not wait for #ipad 2 also . they should sale them down at #sxsw .\n",
       "3                                                                      @sxsw i hope this year's festival isn't as crashy as this year's iphone app . #sxsw\n",
       "4                @sxtxstate great stuff on fri #sxsw : marissa mayer ( google ) , tim o'reilly ( tech books / conferences ) & matt mullenweg ( wordpress )\n",
       "                                                                               ...                                                                        \n",
       "9088                                                                                                                      ipad everywhere . #sxsw { link }\n",
       "9089                      wave , buzz ... rt @mention we interrupt your regularly scheduled #sxsw geek programming with big news { link } #google #circles\n",
       "9090       google's zeiger , a physician never reported potential ae . yet fda relies on physicians . \" we're operating w / out data . \" #sxsw #health2dev\n",
       "9091         some verizon iphone customers complained their time fell back an hour this weekend . of course they were the new yorkers who attended #sxsw .\n",
       "9092     ï ¡  ïà  ü_   ê   î   ò   £   á  ââ   _   £     â_  ûârt @mention google tests  ûïcheck-in offers  û  at #sxsw { link }\n",
       "Name: tweet_text, Length: 9092, dtype: object"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = TweetTokenizer(preserve_case=False,reduce_len=True)\n",
    "tweeted = df['tweet_text'].apply(tweets.tokenize)\n",
    "tweeted.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeted = tweeted.str.replace(r'[^\\w\\s]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      NaN\n",
       "2      NaN\n",
       "3      NaN\n",
       "4      NaN\n",
       "        ..\n",
       "9088   NaN\n",
       "9089   NaN\n",
       "9090   NaN\n",
       "9091   NaN\n",
       "9092   NaN\n",
       "Name: tweet_text, Length: 9092, dtype: float64"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out puncuation and other unnecessary characters\n",
    "df['tweet_text'] = df['tweet_text'].str.replace(r'[^\\w\\s]', '')\n",
    "df['tweet_text'] = df['tweet_text'].str.replace('quot', '')\n",
    "# Make lower case and split\n",
    "df['tweet_text'] = df['tweet_text'].str.lower()\n",
    "df['tweet_text'] = df['tweet_text'].str.split(' ')\n",
    "# Drop the 1 NA tweet we have\n",
    "df = df.dropna(subset=['tweet_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do some basic cleaning of the data set, removing puncuation making everything lower case and splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a blank column to set up for some data cleaning\n",
    "df['test'] = ''\n",
    "# Fill in the values with a string na to avoid errors\n",
    "df.fillna('na',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This salvages alot of our data\n",
    "\n",
    "apple = ['iphone','ipad','apple','apples','ipads']\n",
    "android = ['android','google','androids','googles']\n",
    "\n",
    "# Loop through the tweet column and look for keywords to assign product to\n",
    "for ind , val in enumerate(df['tweet_text']):\n",
    "    if df['emotion_in_tweet_is_directed_at'].values[ind] == 'na':\n",
    "        for y in val:\n",
    "            if y in apple:\n",
    "                df['test'].values[ind] = y\n",
    "            elif y in android:\n",
    "                df['test'].values[ind] = y\n",
    "                \n",
    "    else:\n",
    "        df['test'].values[ind] = df['emotion_in_tweet_is_directed_at'].values[ind]\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there were such a high amount of NA values in the emotion tweeted at column, we thought it was best to try to salvage as much data as possible. We looked for keywords in the tweets and assigned a product to them based on what they "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iphone', 'ipad or iphone app', 'ipad', 'google', 'android',\n",
       "       'apple', 'android app', 'other google product or service', '',\n",
       "       'googles', 'ipads', 'apples', 'other apple product or service',\n",
       "       'androids'], dtype=object)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make everything lower case to make values match\n",
    "df['test'] = df['test'].map(lambda x : x.lower())\n",
    "df['test'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell replaces words with the rightful product to reduce unique values\n",
    "df['test'] = df['test'].replace({'googles': 'google', 'apples': 'apple', 'androids': 'android','ipads':'ipad'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iphone', 'ipad or iphone app', 'ipad', 'google', 'android',\n",
       "       'apple', 'android app', 'other google product or service', '',\n",
       "       'other apple product or service'], dtype=object)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['test'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from 5802 NA values to almost under 900! The remainder tweets do not mention anything about either product, and therefore are not useful for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion',\n",
       "       'No emotion toward brand or product', \"I can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_there_an_emotion_directed_at_a_brand_or_product'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the target of our dataset, there seems to be four different values. Negative emotion, positive emotion, no emotion and I can't tell. Let's get a closer look at these 'I can't tell' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>[thanks, to, mention, for, publishing, the, news, of, mention, new, medical, apps, at, the, sxswi, conf, blog, link, sxsw, sxswh]</td>\n",
       "      <td>na</td>\n",
       "      <td>I can't tell</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>[ûïmention, apple, has, opened, a, popup, store, in, austin, so, the, nerds, in, town, for, sxsw, can, get, their, new, ipads, link, wow]</td>\n",
       "      <td>na</td>\n",
       "      <td>I can't tell</td>\n",
       "      <td>ipad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>[just, what, america, needs, rt, mention, google, to, launch, major, new, social, network, called, circles, possibly, today, link, sxsw]</td>\n",
       "      <td>na</td>\n",
       "      <td>I can't tell</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>[the, queue, at, the, apple, store, in, austin, is, four, blocks, long, crazy, stuff, sxsw]</td>\n",
       "      <td>na</td>\n",
       "      <td>I can't tell</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>[hope, its, better, than, wave, rt, mention, buzz, is, googles, previewing, a, social, networking, platform, at, sxsw, link]</td>\n",
       "      <td>na</td>\n",
       "      <td>I can't tell</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9020</th>\n",
       "      <td>[its, funny, watching, a, room, full, of, people, hold, their, ipad, in, the, air, to, take, a, photo, like, a, room, full, of, tablets, staring, you, down, sxsw]</td>\n",
       "      <td>na</td>\n",
       "      <td>I can't tell</td>\n",
       "      <td>ipad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9032</th>\n",
       "      <td>[mention, yeah, we, have, mention, , google, has, nothing, on, us, , sxsw]</td>\n",
       "      <td>na</td>\n",
       "      <td>I can't tell</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9037</th>\n",
       "      <td>[mention, yes, the, google, presentation, was, not, exactly, what, i, was, expecting, sxsw]</td>\n",
       "      <td>na</td>\n",
       "      <td>I can't tell</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9058</th>\n",
       "      <td>[do, you, know, what, apple, is, really, good, at, making, you, feel, bad, about, your, xmas, present, , seth, meyers, on, ipad2, sxsw, doyoureallyneedthat]</td>\n",
       "      <td>na</td>\n",
       "      <td>I can't tell</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9066</th>\n",
       "      <td>[how, much, you, want, to, bet, apple, is, disproportionately, stocking, the, sxsw, popup, store, with, ipad, 2, the, influencerhipsters, thank, you]</td>\n",
       "      <td>Apple</td>\n",
       "      <td>I can't tell</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                              tweet_text  \\\n",
       "90                                     [thanks, to, mention, for, publishing, the, news, of, mention, new, medical, apps, at, the, sxswi, conf, blog, link, sxsw, sxswh]   \n",
       "102                            [ûïmention, apple, has, opened, a, popup, store, in, austin, so, the, nerds, in, town, for, sxsw, can, get, their, new, ipads, link, wow]   \n",
       "237                             [just, what, america, needs, rt, mention, google, to, launch, major, new, social, network, called, circles, possibly, today, link, sxsw]   \n",
       "341                                                                          [the, queue, at, the, apple, store, in, austin, is, four, blocks, long, crazy, stuff, sxsw]   \n",
       "368                                         [hope, its, better, than, wave, rt, mention, buzz, is, googles, previewing, a, social, networking, platform, at, sxsw, link]   \n",
       "...                                                                                                                                                                  ...   \n",
       "9020  [its, funny, watching, a, room, full, of, people, hold, their, ipad, in, the, air, to, take, a, photo, like, a, room, full, of, tablets, staring, you, down, sxsw]   \n",
       "9032                                                                                          [mention, yeah, we, have, mention, , google, has, nothing, on, us, , sxsw]   \n",
       "9037                                                                         [mention, yes, the, google, presentation, was, not, exactly, what, i, was, expecting, sxsw]   \n",
       "9058        [do, you, know, what, apple, is, really, good, at, making, you, feel, bad, about, your, xmas, present, , seth, meyers, on, ipad2, sxsw, doyoureallyneedthat]   \n",
       "9066               [how, much, you, want, to, bet, apple, is, disproportionately, stocking, the, sxsw, popup, store, with, ipad, 2, the, influencerhipsters, thank, you]   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "90                                na   \n",
       "102                               na   \n",
       "237                               na   \n",
       "341                               na   \n",
       "368                               na   \n",
       "...                              ...   \n",
       "9020                              na   \n",
       "9032                              na   \n",
       "9037                              na   \n",
       "9058                              na   \n",
       "9066                           Apple   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product    test  \n",
       "90                                         I can't tell          \n",
       "102                                        I can't tell    ipad  \n",
       "237                                        I can't tell  google  \n",
       "341                                        I can't tell   apple  \n",
       "368                                        I can't tell  google  \n",
       "...                                                 ...     ...  \n",
       "9020                                       I can't tell    ipad  \n",
       "9032                                       I can't tell  google  \n",
       "9037                                       I can't tell  google  \n",
       "9058                                       I can't tell   apple  \n",
       "9066                                       I can't tell   apple  \n",
       "\n",
       "[156 rows x 4 columns]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['is_there_an_emotion_directed_at_a_brand_or_product'] == \"I can't tell\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this makes up such a small amount of our data, we will move the I can't tell value to that of neutral, along with the value 'No emotion toward brand or product'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make change targets into more readable text\n",
    "emotion_dict = {'No emotion toward brand or product' : 'Neutral emotion', \"I can't tell\": 'Neutral emotion', 'Positive emotion': 'Positive emotion',\n",
    "               'Negative emotion': 'Negative emotion'}\n",
    "df['is_there_an_emotion_directed_at_a_brand_or_product'] = df['is_there_an_emotion_directed_at_a_brand_or_product'].map(emotion_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion', 'Neutral emotion'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_there_an_emotion_directed_at_a_brand_or_product'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['product'] = df['test'].map(company_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>test</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[ûïmention, link, lt, help, me, forward, this, doc, to, all, anonymous, accounts, techiesamp, ppl, who, can, help, us, jam, libya, sxsw]</td>\n",
       "      <td>na</td>\n",
       "      <td>Neutral emotion</td>\n",
       "      <td></td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>[¼, what, _, link, ã_, edchat, musedchat, sxsw, sxswi, classical, newtwitter]</td>\n",
       "      <td>na</td>\n",
       "      <td>Neutral emotion</td>\n",
       "      <td></td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[mention, mention, on, the, locationbased, fast, fun, and, future, , link, via, mention, sxsw]</td>\n",
       "      <td>na</td>\n",
       "      <td>Neutral emotion</td>\n",
       "      <td></td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>[at, sxsw, mention, , mention, wanna, buy, you, a, drink, 7pm, at, fado, on, 4th, link, join, us]</td>\n",
       "      <td>na</td>\n",
       "      <td>Neutral emotion</td>\n",
       "      <td></td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>[chilcott, mention, sxsw, stand, talking, with, blogger, staff, too, late, to, win, competition, for, best, tweet, mentioning, mention, so, no, tshirt]</td>\n",
       "      <td>na</td>\n",
       "      <td>Neutral emotion</td>\n",
       "      <td></td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8932</th>\n",
       "      <td>[z6, no, news, is, good, news, link, codes, valid, 40075959p, 031111, infektd, sxsw, zlf]</td>\n",
       "      <td>na</td>\n",
       "      <td>Neutral emotion</td>\n",
       "      <td></td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8936</th>\n",
       "      <td>[client, news, mention, releases, dope, melodies, amp, heavy, bass, amp, invades, sxsw, gt, link]</td>\n",
       "      <td>na</td>\n",
       "      <td>Neutral emotion</td>\n",
       "      <td></td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8970</th>\n",
       "      <td>[this, is, my, 5th, year, downloading, the, sxsw, music, torrent, link, all, free, and, legal, great, music]</td>\n",
       "      <td>na</td>\n",
       "      <td>Neutral emotion</td>\n",
       "      <td></td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9024</th>\n",
       "      <td>[by, the, way, were, looking, for, a, spanishspeaking, trend, scout, based, in, austin, gt, link, sxsw]</td>\n",
       "      <td>na</td>\n",
       "      <td>Neutral emotion</td>\n",
       "      <td></td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9026</th>\n",
       "      <td>[true, story, rt, mention, i, just, rated, amys, ice, cream, 5, stars, mention, best, ice, cream, in, town, link, sxsw]</td>\n",
       "      <td>na</td>\n",
       "      <td>Neutral emotion</td>\n",
       "      <td></td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                   tweet_text  \\\n",
       "51                   [ûïmention, link, lt, help, me, forward, this, doc, to, all, anonymous, accounts, techiesamp, ppl, who, can, help, us, jam, libya, sxsw]   \n",
       "52                                                                              [¼, what, _, link, ã_, edchat, musedchat, sxsw, sxswi, classical, newtwitter]   \n",
       "53                                                             [mention, mention, on, the, locationbased, fast, fun, and, future, , link, via, mention, sxsw]   \n",
       "66                                                          [at, sxsw, mention, , mention, wanna, buy, you, a, drink, 7pm, at, fado, on, 4th, link, join, us]   \n",
       "71    [chilcott, mention, sxsw, stand, talking, with, blogger, staff, too, late, to, win, competition, for, best, tweet, mentioning, mention, so, no, tshirt]   \n",
       "...                                                                                                                                                       ...   \n",
       "8932                                                                [z6, no, news, is, good, news, link, codes, valid, 40075959p, 031111, infektd, sxsw, zlf]   \n",
       "8936                                                        [client, news, mention, releases, dope, melodies, amp, heavy, bass, amp, invades, sxsw, gt, link]   \n",
       "8970                                             [this, is, my, 5th, year, downloading, the, sxsw, music, torrent, link, all, free, and, legal, great, music]   \n",
       "9024                                                  [by, the, way, were, looking, for, a, spanishspeaking, trend, scout, based, in, austin, gt, link, sxsw]   \n",
       "9026                                  [true, story, rt, mention, i, just, rated, amys, ice, cream, 5, stars, mention, best, ice, cream, in, town, link, sxsw]   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "51                                na   \n",
       "52                                na   \n",
       "53                                na   \n",
       "66                                na   \n",
       "71                                na   \n",
       "...                              ...   \n",
       "8932                              na   \n",
       "8936                              na   \n",
       "8970                              na   \n",
       "9024                              na   \n",
       "9026                              na   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product test  product  \n",
       "51                                      Neutral emotion       unknown  \n",
       "52                                      Neutral emotion       unknown  \n",
       "53                                      Neutral emotion       unknown  \n",
       "66                                      Neutral emotion       unknown  \n",
       "71                                      Neutral emotion       unknown  \n",
       "...                                                 ...  ...      ...  \n",
       "8932                                    Neutral emotion       unknown  \n",
       "8936                                    Neutral emotion       unknown  \n",
       "8970                                    Neutral emotion       unknown  \n",
       "9024                                    Neutral emotion       unknown  \n",
       "9026                                    Neutral emotion       unknown  \n",
       "\n",
       "[915 rows x 5 columns]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['product'] == 'unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = {'Negative emotion': 2, 'Neutral emotion': 1, 'Positive emotion': 0}\n",
    "df_test = df.copy()\n",
    "df_test['target'] = df['is_there_an_emotion_directed_at_a_brand_or_product'].map(target_dict)\n",
    "#df_test = df_test.loc[df_test['product'] != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(['emotion_in_tweet_is_directed_at','test','is_there_an_emotion_directed_at_a_brand_or_product'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_test['tweet_text'].str.join(' ')\n",
    "new_X = X.apply(tweets.tokenize)\n",
    "y = df_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeted =tweeted.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(new_X.str.join(' '),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Our test and train set\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first model we will try a CountVectorizer with Naive Bayes\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_t_vec = cv.fit_transform(X_train)\n",
    "X_t_vec = cv.fit_transform(X_train)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_train.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_vec = cv.transform(X_test)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_test.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(X_t_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_validate(mnb,X_t_vec,y_train,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.8307301743671898\n",
      "Mean Test Score: 0.6653472695860219\n"
     ]
    }
   ],
   "source": [
    "mean_scores(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our first look at the model, it seems to be overfit. It has high variance from our test and train scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = cross_val_predict(mnb,X_t_vec,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.665346825047661\n",
      "Precision Score: 0.665346825047661\n",
      "Accuracy Score:0.665346825047661\n",
      "F1 Score: 0.665346825047661\n",
      "Confusion Matrix: \n",
      "[[1227  995   12]\n",
      " [ 832 3267   59]\n",
      " [ 165  219   43]]\n"
     ]
    }
   ],
   "source": [
    "metrics(y_train,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.9230092317575685\n",
      "Mean Test Score: 0.6893942979842206\n"
     ]
    }
   ],
   "source": [
    "lr_pipe = Pipeline([(\"vec\" , CountVectorizer(stop_words='english')),\n",
    "                       (\"lr\",LogisticRegression())])\n",
    "results_LR = cross_validate(lr_pipe,X_train,y_train,return_train_score=True)\n",
    "mean_scores(results_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.6893972723273207\n",
      "Precision Score: 0.6893972723273207\n",
      "Accuracy Score:0.6893972723273207\n",
      "F1 Score: 0.6893972723273207\n",
      "Confusion Matrix: \n",
      "[[1144 1071   19]\n",
      " [ 617 3483   58]\n",
      " [  87  266   74]]\n"
     ]
    }
   ],
   "source": [
    "y_hat_train = cross_val_predict(lr_pipe, X_train, y_train)\n",
    "metrics(y_train,y_hat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(max_features=500), MultinomialNB())\n",
    "cv = cross_validate(pipe, X_train, y_train,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.6849611389911056\n",
      "Mean Test Score: 0.655814218678394\n"
     ]
    }
   ],
   "source": [
    "mean_scores(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.9967737164376087\n",
      "Mean Test Score: 0.6737052560011876\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(), RandomForestClassifier())\n",
    "cv = cross_validate(pipe, X_train, y_train,return_train_score=True)\n",
    "mean_scores(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a TFidfVectorizer, we have fixed our over fitting problem but the test score has not improved that much. Maybe we can tweak some hyperparameters to help improve the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pipe = Pipeline([( \"tfid\" , TfidfVectorizer(max_features=500)),\n",
    "                       (\"rf\",RandomForestClassifier())])\n",
    "\n",
    "params = {'rf__criterion':['gini', 'entropy'],\n",
    "          'rf__max_depth':[None,3,5],\n",
    "          'rf__min_samples_split':[2,5,10],\n",
    "          'rf__min_samples_leaf':[1,3,5,7],\n",
    "          'rf__max_features' : [5,10]}\n",
    "\n",
    "clf = GridSearchCV(RF_pipe,params,return_train_score = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('tfid',\n",
       "                                        TfidfVectorizer(max_features=500)),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             param_grid={'rf__criterion': ['gini', 'entropy'],\n",
       "                         'rf__max_depth': [None, 3, 5],\n",
       "                         'rf__max_features': [5, 10],\n",
       "                         'rf__min_samples_leaf': [1, 3, 5, 7],\n",
       "                         'rf__min_samples_split': [2, 5, 10]},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-9772bec5683a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.6646375221793046'\n",
      "Mean Test Score: 0.6230672320021744\n"
     ]
    }
   ],
   "source": [
    "gs_mean_scores(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.7158309253798742\n",
      "Mean Test Score: 0.6695995765765959\n"
     ]
    }
   ],
   "source": [
    "lr_pipe = Pipeline([(\"tfid\" , TfidfVectorizer(max_features=500,stop_words='english')),\n",
    "                       (\"lr\",LogisticRegression())])\n",
    "results_LR = cross_validate(lr_pipe,X_train,y_train,return_train_score=True)\n",
    "mean_scores(results_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid = {'lr__C' : [.5, 1, 50],\n",
    "       'lr__max_iter' : [50,100,1750, 2000],\n",
    "       }\n",
    "\n",
    "clf_lr = GridSearchCV(lr_pipe,lr_grid,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_results = clf_LR.fit(X_train,y_train)\n",
    "gs_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.7188860919296151'\n",
      "Mean Test Score: 0.663917534275852\n"
     ]
    }
   ],
   "source": [
    "lr_results = clf_lr.fit(X_train,y_train)\n",
    "gs_mean_scores(lr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 0.5, 'lr__max_iter': 100}"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = ['sxsw', 'mention', 'link', 'ipad', 'iphone', 'google', 'apple', '2', 'android', 'rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.9071344726649123\n",
      "Mean Test Score: 0.6808911900822535\n"
     ]
    }
   ],
   "source": [
    "sgd_pipe = Pipeline([(\"tfid\" , TfidfVectorizer(stop_words=common_words + sw.words('english'))),\n",
    "                       (\"sgd\",SGDClassifier())])\n",
    "results_sgd = cross_validate(sgd_pipe,X_train,y_train,return_train_score=True)\n",
    "mean_scores(results_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sgd = clf_sgd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.6811849244757295\n",
      "Precision Score: 0.6811849244757295\n",
      "Accuracy Score:0.6811849244757295\n",
      "F1 Score: 0.6811849244757295\n",
      "Confusion Matrix: \n",
      "[[1143 1071   20]\n",
      " [ 686 3416   56]\n",
      " [  91  250   86]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = cross_val_predict(sgd_pipe,X_train,y_train)\n",
    "metrics(y_train,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.7479162500347192'\n",
      "Mean Test Score: 0.6479433592307234\n"
     ]
    }
   ],
   "source": [
    "gs_mean_scores(results_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.6800117319255022\n",
      "Precision Score: 0.6800117319255022\n",
      "Accuracy Score:0.6800117319255022\n",
      "F1 Score: 0.6800117319255022\n",
      "Confusion Matrix: \n",
      "[[1131 1083   20]\n",
      " [ 682 3421   55]\n",
      " [  92  250   85]]\n"
     ]
    }
   ],
   "source": [
    "y_hat_train = cross_val_predict(sgd_pipe, X_train, y_train)\n",
    "metrics(y_train,y_hat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.8929828562673541\n",
      "Mean Test Score: 0.6776643078597969\n"
     ]
    }
   ],
   "source": [
    "xgb_pipe = Pipeline([(\"tfid\" , TfidfVectorizer()),\n",
    "                       (\"xgb\",XGBClassifier())])\n",
    "results_xgb = cross_validate(xgb_pipe,X_train,y_train,return_train_score=True)\n",
    "mean_scores(results_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.8431220650967258\n",
      "Mean Test Score: 0.6914468687538056\n"
     ]
    }
   ],
   "source": [
    "xgb_pipe = Pipeline([(\"count\" , CountVectorizer()),\n",
    "                       (\"xgb\",XGBClassifier())])\n",
    "results_xgb = cross_validate(xgb_pipe,X_train,y_train,return_train_score=True)\n",
    "mean_scores(results_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = {'xgb__n_estimators':[50,75,100],\n",
    "            'xgb__max_depth':[1,3,4,5]}\n",
    "\n",
    "xgb_gs = GridSearchCV(estimator=xgb_pipe, param_grid=xgb_grid, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_xgb = xgb_gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.744308088013275'\n",
      "Mean Test Score: 0.673068202437123\n"
     ]
    }
   ],
   "source": [
    "gs_mean_scores(results_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = cross_val_predict(xgb_gs.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.6870508872268661\n",
      "Precision Score: 0.6870508872268661\n",
      "Accuracy Score:0.6870508872268661\n",
      "F1 Score: 0.6870508872268661\n",
      "Confusion Matrix: \n",
      "[[ 877 1340   17]\n",
      " [ 394 3733   31]\n",
      " [  61  291   75]]\n"
     ]
    }
   ],
   "source": [
    "metrics(y_train,xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
